{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Q3i-claevDgP","yjxEybcOvS8O","ORIK6k3yveOY"],"authorship_tag":"ABX9TyOX/M1A3W8d1XIKBbTF7IF1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Libraries"],"metadata":{"id":"zsgGaeysvvCI"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms"],"metadata":{"id":"kekt9lcGuIEA","executionInfo":{"status":"ok","timestamp":1705810107517,"user_tz":-330,"elapsed":13598,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["#Basic autograd example 1"],"metadata":{"id":"X1ObAQc2uPsy"}},{"cell_type":"code","source":["# Create tensors.\n","x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(3., requires_grad=True)\n","\n","# Build a computational graph.\n","y = w * x + b    # y = 2 * x + 3\n","\n","# Compute gradients.\n","y.backward()\n","\n","# Print out the gradients.\n","print(x.grad)    # x.grad = 2\n","print(w.grad)    # w.grad = 1\n","print(b.grad)    # b.grad = 1"],"metadata":{"id":"rEgA5cAhuOyb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705810107518,"user_tz":-330,"elapsed":20,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}},"outputId":"7d6c538e-4912-4889-baf9-8024bc7887b8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(2.)\n","tensor(1.)\n","tensor(1.)\n"]}]},{"cell_type":"markdown","source":["#Basic autograd example 2"],"metadata":{"id":"cHs65IFQua2G"}},{"cell_type":"code","source":["# Create tensors of shape (10, 3) and (10, 2).\n","x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n","\n","# Build a fully connected layer.\n","linear = nn.Linear(3, 2)\n","print ('w: ', linear.weight)\n","print ('b: ', linear.bias)\n","\n","# Build loss function and optimizer.\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n","\n","# Forward pass.\n","pred = linear(x)\n","\n","# Compute loss.\n","loss = criterion(pred, y)\n","print('loss: ', loss.item())\n","\n","# Backward pass.\n","loss.backward()\n","\n","# Print out the gradients.\n","print ('dL/dw: ', linear.weight.grad)\n","print ('dL/db: ', linear.bias.grad)\n","\n","# 1-step gradient descent.\n","optimizer.step()\n","\n","# You can also perform gradient descent at the low level.\n","# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n","# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n","\n","# Print out the loss after 1-step gradient descent.\n","pred = linear(x)\n","loss = criterion(pred, y)\n","print('loss after 1 step optimization: ', loss.item())"],"metadata":{"id":"6WYewkLcul40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705810107518,"user_tz":-330,"elapsed":8,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}},"outputId":"3d1018b8-ff6c-4982-e513-64f90ab93f90"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["w:  Parameter containing:\n","tensor([[-0.4663,  0.0355,  0.2281],\n","        [ 0.5725, -0.5605,  0.1079]], requires_grad=True)\n","b:  Parameter containing:\n","tensor([ 0.3784, -0.4329], requires_grad=True)\n","loss:  2.046734094619751\n","dL/dw:  tensor([[-0.1716,  0.1005,  0.2378],\n","        [ 0.1678, -0.5876,  0.0353]])\n","dL/db:  tensor([ 0.4999, -1.0242])\n","loss after 1 step optimization:  2.02915096282959\n"]}]},{"cell_type":"markdown","source":["#Loading data from numpy"],"metadata":{"id":"u7y4MlSpuoSN"}},{"cell_type":"code","source":["# Create a numpy array.\n","x = np.array([[1, 2], [3, 4]])\n","\n","# Convert the numpy array to a torch tensor.\n","y = torch.from_numpy(x)\n","\n","# Convert the torch tensor to a numpy array.\n","z = y.numpy()"],"metadata":{"id":"z3P5FuXJuo74","executionInfo":{"status":"ok","timestamp":1705810107518,"user_tz":-330,"elapsed":6,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#Input pipeline"],"metadata":{"id":"HO3Guwecu246"}},{"cell_type":"code","source":["# Download and construct CIFAR-10 dataset.\n","train_dataset = torchvision.datasets.CIFAR10(root='/content/data',\n","                                             train=True,\n","                                             transform=transforms.ToTensor(),\n","                                             download=True)\n","\n","# Fetch one data pair (read data from disk).\n","image, label = train_dataset[0]\n","print (image.size())\n","print (label)\n","\n","# Data loader (this provides queues and threads in a very simple way).\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=64,\n","                                           shuffle=True)\n","\n","# When iteration starts, queue and thread start to load data from files.\n","data_iter = iter(train_loader)\n","\n","# Mini-batch images and labels.\n","#images, labels = data_iter.next()\n","images, labels = next(data_iter)\n","\n","# Actual usage of the data loader is as below.\n","for images, labels in train_loader:\n","    # Training code should be written here.\n","    pass"],"metadata":{"id":"lV7X87jxu1gU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705810129904,"user_tz":-330,"elapsed":22391,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}},"outputId":"4bf27a44-478d-4b41-8d88-a3a80dba83ca"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:02<00:00, 68259801.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /content/data/cifar-10-python.tar.gz to /content/data\n","torch.Size([3, 32, 32])\n","6\n"]}]},{"cell_type":"markdown","source":["#Input pipeline for custom dataset"],"metadata":{"id":"Q3i-claevDgP"}},{"cell_type":"code","source":["# You should build your custom dataset as below.\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self):\n","        # TODO\n","        # 1. Initialize file paths or a list of file names.\n","        pass\n","    def __getitem__(self, index):\n","        # TODO\n","        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n","        # 2. Preprocess the data (e.g. torchvision.Transform).\n","        # 3. Return a data pair (e.g. image and label).\n","        pass\n","    def __len__(self):\n","        # You should change 0 to the total size of your dataset.\n","        return 0\n","\n","# class CustomImageDataset(Dataset):\n","#     def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","#         self.img_labels = pd.read_csv(annotations_file)\n","#         self.img_dir = img_dir\n","#         self.transform = transform\n","#         self.target_transform = target_transform\n","\n","#     def __len__(self):\n","#         return len(self.img_labels)\n","\n","#     def __getitem__(self, idx):\n","#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","#         image = read_image(img_path)\n","#         label = self.img_labels.iloc[idx, 1]\n","#         if self.transform:\n","#             image = self.transform(image)\n","#         if self.target_transform:\n","#             label = self.target_transform(label)\n","#         return image, label\n","\n","# You can then use the prebuilt data loader.\n","# custom_dataset = CustomDataset()\n","# train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n","#                                            batch_size=64,\n","#                                            shuffle=True)\n"],"metadata":{"id":"s280qN8bvChe","executionInfo":{"status":"ok","timestamp":1705810129904,"user_tz":-330,"elapsed":17,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#Pretrained model"],"metadata":{"id":"yjxEybcOvS8O"}},{"cell_type":"code","source":["# Download and load the pretrained ResNet-18.\n","resnet = torchvision.models.resnet18(pretrained=True)\n","\n","# If you want to finetune only the top layer of the model, set as below.\n","for param in resnet.parameters():\n","    param.requires_grad = False\n","\n","# Replace the top layer for finetuning.\n","resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n","\n","# Forward pass.\n","images = torch.randn(64, 3, 224, 224)\n","outputs = resnet(images)\n","print (outputs.size())     # (64, 100)"],"metadata":{"id":"Z0lRNH27vStG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705810137889,"user_tz":-330,"elapsed":7989,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}},"outputId":"8cafc055-12a8-496f-bd29-941d71f59d56"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 117MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([64, 100])\n"]}]},{"cell_type":"markdown","source":["#Save and load the model"],"metadata":{"id":"ORIK6k3yveOY"}},{"cell_type":"code","source":["# Save and load the entire model.\n","torch.save(resnet, 'model.ckpt')\n","model = torch.load('model.ckpt')\n","\n","# Save and load only the model parameters (recommended).\n","torch.save(resnet.state_dict(), 'params.ckpt')\n","resnet.load_state_dict(torch.load('params.ckpt'))"],"metadata":{"id":"ct0dcFtvvd-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705810138554,"user_tz":-330,"elapsed":678,"user":{"displayName":"Nivedita Sethiya","userId":"08503905711289493690"}},"outputId":"d8cd313c-9d31-4ee5-90c2-d09e90d0e967"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":8}]}]}